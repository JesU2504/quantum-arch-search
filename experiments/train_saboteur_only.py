import gymnasium as gym
import qas_gym
import numpy as np
from stable_baselines3 import PPO
import cirq
import matplotlib.pyplot as plt
from stable_baselines3.common.callbacks import BaseCallback
import os
import logging

from experiments import config
from qas_gym.utils import get_ghz_state

logging.getLogger('stable_baselines3').setLevel(logging.WARNING)

class ProgressCallback(BaseCallback):
    def __init__(self, verbose=0):
        super(ProgressCallback, self).__init__(verbose)
        self.fidelities = []
        # Record the training timestep when each episode/fidelity is recorded
        self.steps = []

    def _on_step(self) -> bool:
        # In modern gymnasium/SB3, it's robust to check for episode end via `dones`
        # and then access the info dictionary.
        if self.locals['dones'][0]:
            info = self.locals['infos'][0]
            # The SaboteurEnv always provides fidelity in its info dict on termination.
            if 'fidelity' in info:
                self.fidelities.append(info['fidelity'])
                # record the timestep for alignment with architect traces
                try:
                    self.steps.append(self.num_timesteps)
                except Exception:
                    # best-effort: if num_timesteps not available, skip
                    pass
        return True

def train_saboteur_only(results_dir, n_qubits, saboteur_steps, n_steps, max_error_level, baseline_circuit_path=None, lambda_penalty=0.5):
    """
    This experiment verifies that the saboteur can learn to attack a static circuit
    generated by the architect.

    Args:
        results_dir (str): The directory to load inputs from and save outputs to.
        n_qubits (int): The number of qubits for this training run.
        saboteur_steps (int): The total number of timesteps to train the agent.
        n_steps (int): The number of steps to run for each environment per update.
        max_error_level (int): The curriculum level for the saboteur.
    """
    print("--- Experiment 3: Verify Saboteur's Learning (Static Architect) ---")
    os.makedirs(results_dir, exist_ok=True)

    # Define file paths based on the results_dir
    circuit_path = os.path.join(results_dir, "circuit_vanilla.json")
    model_path = os.path.join(results_dir, "saboteur_trained_on_architect_model.zip")
    plot_path = os.path.join(results_dir, "saboteur_trained_on_architect_ghz_training_progress.png")
    fidelities_path = os.path.join(results_dir, "saboteur_trained_on_architect_ghz_fidelities.txt")
    steps_path = os.path.join(results_dir, "saboteur_trained_on_architect_ghz_steps.txt")

    # 1. Load the architect's circuit. Allow an explicit baseline path to be passed
    # (useful when orchestrator stores baseline in a sibling folder).
    try:
        path_to_load = baseline_circuit_path or circuit_path
        if not os.path.exists(path_to_load):
            print(f"Error: {path_to_load} not found. Please run train_architect_ghz.py first for this run.")
            return
        from qas_gym.utils import load_circuit
        static_circuit = load_circuit(path_to_load)

        if not static_circuit.all_operations():
            print(f"Error: The loaded circuit from {path_to_load} is empty. Cannot train saboteur.")
            return
    except FileNotFoundError:
        print(f"Error: {path_to_load} not found. Please run train_architect_ghz.py first.")
        return

    target_state = get_ghz_state(n_qubits)

    print("Architect's Circuit:")
    print(static_circuit)
    print("\n")

    # 2. Create a SaboteurMultiGateEnv with this circuit
    from qas_gym.envs.saboteur_env import SaboteurMultiGateEnv
    # Set max_error_level to 4 for more action diversity
    env = SaboteurMultiGateEnv(
        architect_circuit=static_circuit,
        target_state=target_state,
        max_error_level=4,  # Use 4 error levels for meaningful action space
        discrete=True,
        episode_length=5,  # Multi-step episodes
        lambda_penalty=lambda_penalty
    )
    print("[SaboteurMultiGateEnv] Action space:", env.action_space)

    # 3. Train the saboteur model
    # Use the centralized agent hyperparameters from the config file
    agent_params = config.AGENT_PARAMS.copy()
    agent_params['n_steps'] = n_steps # Override n_steps with the value from the per-qubit config
    agent_params['ent_coef'] = 0.05  # Lower entropy for more policy refinement after initial exploration

    # Enhanced logger to track action distribution and rewards
    from collections import Counter
    class ActionLoggerCallback(BaseCallback):
        def __init__(self, verbose=0, log_interval=1000):
            super().__init__(verbose)
            self.action_log = []
            self.reward_log = []
            self.last_policy = None
            self.log_interval = log_interval
            self.last_logged_step = 0
        def _on_step(self) -> bool:
            actions = self.locals.get('actions', None)
            rewards = self.locals.get('rewards', None)
            if actions is not None:
                self.action_log.append(tuple(actions[0]) if isinstance(actions[0], (list, np.ndarray)) else actions)
            if rewards is not None:
                self.reward_log.append(rewards[0] if isinstance(rewards, (list, np.ndarray)) else rewards)
            # Print action distribution every log_interval steps
            if self.num_timesteps - self.last_logged_step >= self.log_interval:
                self.last_logged_step = self.num_timesteps
                if self.action_log:
                    flat_actions = [str(a) for a in self.action_log]
                    action_counts = Counter(flat_actions)
                    print(f"[Saboteur PPO] Action distribution at step {self.num_timesteps}: {action_counts}")
                if self.reward_log:
                    avg_reward = np.mean(self.reward_log[-self.log_interval:]) if len(self.reward_log) >= self.log_interval else np.mean(self.reward_log)
                    print(f"[Saboteur PPO] Average reward (last {self.log_interval}): {avg_reward:.4f}")
            # Print only when the policy is actually updated (changed == True)
            if rewards is not None and hasattr(self.model, 'policy'):
                current_policy = self.model.policy.state_dict() if hasattr(self.model.policy, 'state_dict') else None
                if self.last_policy is not None and current_policy is not None:
                    changed = any((self.last_policy[k] != v).any() for k, v in current_policy.items() if k in self.last_policy)
                    if changed:
                        print(f"[Saboteur PPO] Policy updated: True")
                self.last_policy = current_policy
            return True

    action_logger = ActionLoggerCallback(verbose=1, log_interval=1000)
    progress_callback = ProgressCallback()
    model = PPO("MlpPolicy", env=env, **agent_params)
    model.learn(total_timesteps=saboteur_steps, callback=[progress_callback, action_logger])

    print("\n--- Training Finished ---")

    # 4. Save the model for analysis
    model.save(model_path)
    print(f"Saved trained saboteur model to {model_path}")

    # 5. Plotting
    window_size = 50  # Lowered for quick runs
    if len(progress_callback.fidelities) >= window_size:
        moving_averages = np.convolve(progress_callback.fidelities, np.ones(window_size)/window_size, mode='valid')
        plt.figure(figsize=(8, 6))
        plt.plot(moving_averages)
        plt.title(f"Fidelity under Saboteur Attack on Vanilla Circuit ({os.path.basename(results_dir)})")
        plt.xlabel(f"Step (window size = {window_size})")
        plt.ylabel("Fidelity (Moving Average)")
        plt.savefig(plot_path)
        print(f"Plot saved to {plot_path}")
    else:
        print(f"Not enough data ({len(progress_callback.fidelities)} points) to generate a plot with window size {window_size}.")

    # Save fidelities to a file for analysis
    with open(fidelities_path, "w") as f:
        for fidelity in progress_callback.fidelities:
            f.write(f"{fidelity}\n")
    print(f"Fidelity data saved to {fidelities_path}")
    # Save saboteur step indices (if recorded)
    try:
        with open(steps_path, "w") as f:
            for s in progress_callback.steps:
                f.write(f"{s}\n")
        print(f"Saboteur step indices saved to {steps_path}")
    except Exception:
        print("Warning: saboteur steps not saved (not recorded or empty)")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--baseline_circuit_path', type=str, default=None, help='Path to architect baseline circuit')
    parser.add_argument('--lambda-penalty', type=float, default=0.5, help='Penalty coefficient for mean error rate in saboteur reward')
    args = parser.parse_args()
    params = config.EXPERIMENT_PARAMS[4] # Default to 4 qubits for standalone run
    train_saboteur_only(
        results_dir="results",
        n_qubits=4,
        saboteur_steps=params["SABOTEUR_STEPS"],
        n_steps=params["SABOTEUR_N_STEPS"],
        max_error_level=4,
        baseline_circuit_path=args.baseline_circuit_path,
        lambda_penalty=args.lambda_penalty
    )
