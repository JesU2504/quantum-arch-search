# Quantum Architecture Search via Adversarial Co‑Evolution

Co‑evolutionary quantum circuit search with two agents: an Architect (constructs circuits) and a Saboteur (injects noise). The adversarial game acts as a parameter‑free, dynamic regularizer that discourages bloat and promotes robustness.

This repository contains a turnkey pipeline to train baseline and adversarial agents, generate co‑evolution plots, and compare robustness of “vanilla” vs “robust” circuits.


## Research goal (for the talk)

We demonstrate that Adversarial Co‑Evolution is a parameter‑free regularizer that outperforms static penalty methods on stability, robustness, and Pareto efficiency. See `ExpPlan.md` for the detailed experimental plan and rationale.


## Install

Tested with Python 3.12 on Linux.

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Dependencies (pinned): Cirq, Gymnasium, Stable‑Baselines3, NumPy, Matplotlib.


## Quick demo (3‑qubit GHZ, full pipeline)

Runs baseline architect training, saboteur‑only training, adversarial co‑evolution for a couple of generations, plotting, and a robustness comparison.

```bash
python run_experiments.py --preset quick
```

Outputs (example): `results/run_YYYYMMDD-HHMMSS/`
- `baseline/`
	- `circuit_vanilla.json` — champion vanilla (noiseless) circuit
	- `architect_ghz_fidelities.txt`, `architect_ghz_steps.txt`, `architect_ghz_training_progress.png`
- `saboteur/`
	- `saboteur_trained_on_architect_model.zip`
	- `saboteur_trained_on_architect_ghz_fidelities.txt`, `saboteur_trained_on_architect_ghz_steps.txt`, `...training_progress.png`
- `adversarial/adversarial_training_*/`
	- `circuit_robust.json` — robust circuit after co‑evolution
	- `coevolution_corrected.png` — corrected co‑evolution plot
- `compare/run_0/`
	- `circuit_vanilla.json`, `circuit_robust.json`
- `compare/`
	- `robust_eval.json`, `attacked_fidelity_samples.csv`, `robustness_comparison.png`


## Reproducing key artifacts

- Co‑evolution plot: produced automatically by the quick pipeline and saved under `adversarial/adversarial_training_*/coevolution_corrected.png`.
- Robustness comparison plot: produced under `compare/robustness_comparison.png` comparing vanilla vs robust circuits under multi‑gate depolarizing attacks.

To scale up, use:

```bash
# Longer runs
python run_experiments.py --preset full --n-qubits 3

# Custom steps (override parts of a preset)
python run_experiments.py --preset quick --baseline-steps 5000 --saboteur-steps 5000 --n-qubits 3
```


## How it works (files you’ll touch)

- Entry point: `run_experiments.py` — orchestrates baseline → saboteur‑only → adversarial → compare.
- Experiment scripts (in `experiments/`):
	- `train_architect_ghz.py`: baseline Architect for GHZ preparation
	- `train_saboteur_only.py`: Saboteur learns to attack the baseline circuit
	- `train_adversarial.py`: co‑evolution between Architect and Saboteur
	- `plot_coevolution.py`: renders corrected co‑evolution plots
	- `compare_circuits.py`: aggregates vanilla vs robust robustness metrics/plots
- Environments (in `src/qas_gym/envs/`):
	- `architect_env.py`: reward‑shaped ArchitectEnv and AdversarialArchitectEnv
	- `saboteur_env.py`: SaboteurMultiGateEnv with attack budget and vectorized noise actions
	- `qas_env.py`: shared circuit/env utilities


## Mapping to the Experimental Plan

The repository ships the core pipeline and figures for Parts 2 and 7, and a strong baseline for Part 3. For completeness, the full plan is in `ExpPlan.md`.

- Part 1 (λ‑sweep brittleness, static penalty): not included as code here. Our method avoids λ entirely; reproducing tuned static‑penalty sweeps would require a simple reward variant (R = Fidelity − λ·Cost) and a small sweep harness.
- Part 2 (Robustness to shift): included — the Saboteur varies attacks during training; robustness comparison is generated by `compare_circuits.py` (multi‑gate depolarizing). Extending to asymmetric/coherent noise sweeps is straightforward in that script.
- Part 3 (Pareto frontier): partially included — we output fidelity and circuits. To plot CNOT‑count vs fidelity, compute CNOT count from `circuit_*.json` and scatter. If desired, we can add explicit CNOT logging in env info.
- Part 4 (VQE on H4): not included in this repository.
- Part 5 (Overhead analysis): not included as an automated benchmark; you can wrap wall‑clock timing around each phase in `run_experiments.py`.
- Part 6 (QEC resource plot): not included in this repository. See `ExpPlan.md` for the argument framing; adding a simple bar‑chart script to visualize physical‑qubit overhead is straightforward.
- Part 7 (Verification): included conceptually — saboteur‑only run demonstrates fidelity degradation and learning. A dedicated `verify_saboteur` script can be added or adapted if you want a one‑shot check on a perfect GHZ circuit.


## Tips and troubleshooting

- Seeds: pass `--seed <int>` to `run_experiments.py` for reproducibility.
- QuBits: use `--n-qubits 3` (default). Higher qubit counts raise simulation cost.
- Results hygiene: archive or prune older runs in `results/` to keep the bundle small.
- Gym vs Gymnasium: this project uses Gymnasium; if you see a warning about Gym, it’s safe to ignore.


## Citation

If you use this code, please cite the corresponding talk/paper. For questions or issues, open an issue or contact the authors.



## Changelog

- **Legacy Environment Directory Removed**: The deprecated `src/envs/` directory (which served as a backward-compatibility shim) has been removed. All environment imports must now use `src.qas_gym.envs` directly.
- **Environment Consolidation**: All environments and agents (ArchitectEnv, AdversarialArchitectEnv, Saboteur, VQEArchitectEnv) are now unified under `src/qas_gym/envs/`; duplicate definitions in `src/envs/` have been removed. Import from `src.qas_gym.envs` for all environment classes.

## Statistical Reporting Protocol

This repository follows best-practice statistical reporting for quantum architecture search experiments. All major experiments support multi-seed runs for statistical validity.

### Configuration

- **Number of seeds**: Configurable via `--n-seeds <int>` (default: 5, recommended: 10 for publication).
- **Seed control**: Use `--seed <int>` to set the base random seed for reproducibility.

### Running Multi-Seed Experiments

```bash
# Run full pipeline with 10 seeds per setting
python run_experiments.py --preset full --n-seeds 10 --seed 42

# Run specific experiment with custom seeds
python experiments/lambda_sweep_ghz.py --n-seeds 10 --n-qubits 4

# Parameter recovery with configurable repetitions
python experiments/parameter_recovery.py --n-repetitions 10 --baseline-circuit path/to/circuit.json
```

### Output Structure

Each experiment produces:
- **Per-seed results**: Individual JSON files for each seed (e.g., `lambda_0.001/seed_0_results.json`)
- **Aggregated results**: Combined statistics with mean +/- std
- **Summary files**:
  - `experiment_summary.json`: Machine-readable summary with all parameters
  - `experiment_summary.txt`: Human-readable summary
- **Plots with error bars**: All plots show mean +/- std with sample size annotation (n=...)

### Statistical Metrics

- **Aggregation method**: Mean +/- std (sample standard deviation with ddof=1)
- **Confidence intervals**: 95% CI using t-distribution
- **Success rate**: Wilson score interval for binomial proportions
- **Plots**: Error bars (mean +/- std), faint individual curves overlay, sample size annotation

### Checklist for Multi-Seed Experiments

1. Set `--n-seeds` to at least 5 (ideally 10)
2. Set `--seed` for reproducibility
3. Verify per-seed results are saved in experiment subdirectories
4. Check summary files contain all seeds used and hyperparameters
5. Confirm plots show error bars and sample size annotations
