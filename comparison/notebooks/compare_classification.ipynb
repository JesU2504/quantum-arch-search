{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL vs EA Quantum Architecture Search: Classification Comparison\n",
    "\n",
    "This notebook provides scaffolding for comparing the DRL approach from arXiv:2407.20147\n",
    "with the coevolutionary (EA) agents on **classification tasks**.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Run classification experiments using configs in `comparison/experiments/configs/`:\n",
    "   - `drl_classification.yaml` for DRL method\n",
    "   - `ea_classification.yaml` for EA method\n",
    "2. Place log files in `comparison/logs/` following naming convention\n",
    "3. Install dependencies: `pip install -r comparison/requirements.txt`\n",
    "\n",
    "## Reference\n",
    "\n",
    "- **Paper**: \"Quantum Machine Learning Architecture Search via Deep Reinforcement Learning\"\n",
    "- **arXiv**: [2407.20147](https://arxiv.org/abs/2407.20147)\n",
    "- **Venue**: IEEE QCE 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repository root to path\n",
    "repo_root = Path().resolve().parent.parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import comparison modules\n",
    "from comparison.analysis.compute_classif_metrics import (\n",
    "    load_logs,\n",
    "    validate_classification_logs,\n",
    "    aggregate_classification_metrics,\n",
    "    save_classification_summary,\n",
    ")\n",
    "\n",
    "# Optional: Import visualization libraries\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    HAS_PLOTTING = True\n",
    "except ImportError:\n",
    "    HAS_PLOTTING = False\n",
    "    print(\"matplotlib/numpy not available. Install with: pip install matplotlib numpy\")\n",
    "\n",
    "# Set plotting style\n",
    "if HAS_PLOTTING:\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    plt.rcParams['figure.figsize'] = (10, 6)\n",
    "    plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Paper Metadata\n",
    "\n",
    "Load the extracted hyperparameters and experimental details from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paper metadata\n",
    "METADATA_PATH = Path(\"../paper_metadata/quantum_ml_arch_search_2407.20147.json\")\n",
    "\n",
    "with open(METADATA_PATH, 'r') as f:\n",
    "    paper_metadata = json.load(f)\n",
    "\n",
    "print(f\"Paper: {paper_metadata['paper_title']}\")\n",
    "print(f\"arXiv: {paper_metadata['arxiv_id']}\")\n",
    "print(f\"Authors: {', '.join(paper_metadata['authors'])}\")\n",
    "print(f\"\\nDRL Algorithm: {paper_metadata['drl_algorithm']['name']}\")\n",
    "print(f\"Gate Set: {paper_metadata['action_space']['gate_set']}\")\n",
    "print(f\"Max Gates: {paper_metadata['max_depth_termination']['max_gates']['L_values_used']}\")\n",
    "print(f\"\\nDatasets:\")\n",
    "for task in paper_metadata['tasks']:\n",
    "    print(f\"  - {task['name']}: {task['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Experiment Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure log paths\n",
    "LOGS_DIR = Path(\"../logs\")\n",
    "\n",
    "# Load DRL classification logs\n",
    "drl_log_pattern = str(LOGS_DIR / \"drl_classification\" / \"*.jsonl\")\n",
    "drl_logs = load_logs(drl_log_pattern)\n",
    "print(f\"Loaded {len(drl_logs)} DRL classification log entries\")\n",
    "\n",
    "# Load EA classification logs\n",
    "ea_log_pattern = str(LOGS_DIR / \"ea_classification\" / \"*.jsonl\")\n",
    "ea_logs = load_logs(ea_log_pattern)\n",
    "print(f\"Loaded {len(ea_logs)} EA classification log entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validate Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate DRL logs\n",
    "valid_drl, drl_errors = validate_classification_logs(drl_logs)\n",
    "print(f\"DRL: {len(valid_drl)} valid, {len(drl_errors)} errors\")\n",
    "\n",
    "# Validate EA logs\n",
    "valid_ea, ea_errors = validate_classification_logs(ea_logs)\n",
    "print(f\"EA: {len(valid_ea)} valid, {len(ea_errors)} errors\")\n",
    "\n",
    "# Show errors if any\n",
    "if drl_errors:\n",
    "    print(\"\\nDRL validation errors (first 3):\")\n",
    "    for idx, err in drl_errors[:3]:\n",
    "        print(f\"  Entry {idx}: {err}\")\n",
    "\n",
    "if ea_errors:\n",
    "    print(\"\\nEA validation errors (first 3):\")\n",
    "    for idx, err in ea_errors[:3]:\n",
    "        print(f\"  Entry {idx}: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine logs for comparison\n",
    "all_logs = valid_drl + valid_ea\n",
    "\n",
    "# Compute aggregated metrics\n",
    "metrics = aggregate_classification_metrics(all_logs)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Total runs: {metrics['total_runs']}\")\n",
    "print(f\"Total log entries: {metrics['total_logs']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION METRICS BY METHOD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method, stats in metrics.get('by_method', {}).items():\n",
    "    print(f\"\\n--- {method.upper()} ---\")\n",
    "    print(f\"  Runs: {stats['n_runs']}\")\n",
    "    \n",
    "    if stats.get('mean_best_test_accuracy') is not None:\n",
    "        std = stats.get('std_best_test_accuracy') or 0\n",
    "        print(f\"  Best test accuracy: {stats['mean_best_test_accuracy']:.4f} ± {std:.4f}\")\n",
    "    \n",
    "    if stats.get('mean_final_test_accuracy') is not None:\n",
    "        std = stats.get('std_final_test_accuracy') or 0\n",
    "        print(f\"  Final test accuracy: {stats['mean_final_test_accuracy']:.4f} ± {std:.4f}\")\n",
    "    \n",
    "    if stats.get('success_rate_90') is not None:\n",
    "        print(f\"  Success rate (≥90%): {stats['success_rate_90']*100:.1f}%\")\n",
    "    \n",
    "    if stats.get('mean_final_gate_count') is not None:\n",
    "        std = stats.get('std_final_gate_count') or 0\n",
    "        print(f\"  Final gate count: {stats['mean_final_gate_count']:.1f} ± {std:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot: Accuracy vs Evaluations\n",
    "\n",
    "Compare learning curves showing classification accuracy improvement over training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_vs_evals(logs_by_method, accuracy_field='test_accuracy',\n",
    "                           title=\"Classification Accuracy vs Evaluations\"):\n",
    "    \"\"\"\n",
    "    Plot accuracy learning curves for each method.\n",
    "    \n",
    "    Args:\n",
    "        logs_by_method: dict mapping method name to list of log entries\n",
    "        accuracy_field: which accuracy field to plot\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    colors = {'drl': '#1f77b4', 'ea': '#ff7f0e'}\n",
    "    \n",
    "    for method, logs in logs_by_method.items():\n",
    "        # Group by seed\n",
    "        seeds = {}\n",
    "        for log in logs:\n",
    "            seed = log.get('seed', 0)\n",
    "            if seed not in seeds:\n",
    "                seeds[seed] = {'evals': [], 'accuracies': []}\n",
    "            \n",
    "            # Get evaluation index\n",
    "            eval_idx = log.get('eval_id') or log.get('episode') or log.get('generation', 0)\n",
    "            seeds[seed]['evals'].append(eval_idx)\n",
    "            \n",
    "            # Get accuracy\n",
    "            acc = log.get(accuracy_field) or log.get('train_accuracy') or log.get('accuracy', 0)\n",
    "            seeds[seed]['accuracies'].append(acc)\n",
    "        \n",
    "        color = colors.get(method, 'gray')\n",
    "        \n",
    "        # Plot each seed with light color\n",
    "        for seed, data in seeds.items():\n",
    "            # Sort by evals\n",
    "            sorted_pairs = sorted(zip(data['evals'], data['accuracies']))\n",
    "            evals, accs = zip(*sorted_pairs) if sorted_pairs else ([], [])\n",
    "            ax.plot(evals, accs, color=color, alpha=0.2, linewidth=1)\n",
    "        \n",
    "        # Compute and plot mean curve\n",
    "        if seeds:\n",
    "            # Interpolate to common x values for averaging\n",
    "            all_evals = sorted(set(e for seed_data in seeds.values() for e in seed_data['evals']))\n",
    "            if all_evals:\n",
    "                mean_accs = []\n",
    "                for eval_pt in all_evals:\n",
    "                    accs_at_pt = []\n",
    "                    for seed_data in seeds.values():\n",
    "                        for e, a in zip(seed_data['evals'], seed_data['accuracies']):\n",
    "                            if e == eval_pt:\n",
    "                                accs_at_pt.append(a)\n",
    "                    mean_accs.append(np.mean(accs_at_pt) if accs_at_pt else np.nan)\n",
    "                \n",
    "                ax.plot(all_evals, mean_accs, color=color, linewidth=2.5,\n",
    "                       label=f\"{method.upper()} (n={len(seeds)})\")\n",
    "    \n",
    "    # Add threshold lines\n",
    "    for thresh in [0.7, 0.8, 0.9]:\n",
    "        ax.axhline(y=thresh, color='gray', linestyle='--', alpha=0.3, linewidth=1)\n",
    "        ax.text(ax.get_xlim()[1] * 0.98, thresh + 0.01, f'{int(thresh*100)}%',\n",
    "               ha='right', va='bottom', color='gray', fontsize=10)\n",
    "    \n",
    "    ax.set_xlabel('Evaluations (episodes/generations)', fontsize=12)\n",
    "    ax.set_ylabel('Classification Accuracy', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(loc='lower right', fontsize=11)\n",
    "    ax.set_ylim(0.4, 1.02)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage (uncomment when logs are available)\n",
    "# fig = plot_accuracy_vs_evals({'drl': valid_drl, 'ea': valid_ea})\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when logs are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plot: ECDF of Final Accuracies\n",
    "\n",
    "Empirical cumulative distribution function comparing final accuracies across seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_ecdf(metrics, title=\"ECDF of Final Classification Accuracies\"):\n",
    "    \"\"\"\n",
    "    Plot empirical CDF of final accuracies for each method.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Aggregated metrics from aggregate_classification_metrics\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    colors = {'drl': '#1f77b4', 'ea': '#ff7f0e'}\n",
    "    \n",
    "    # Extract final accuracies from per_run metrics\n",
    "    for method in ['drl', 'ea']:\n",
    "        accuracies = []\n",
    "        for run_key, run_data in metrics.get('per_run', {}).items():\n",
    "            if run_data['method'] == method:\n",
    "                acc = run_data.get('final_test_accuracy') or run_data.get('best_test_accuracy')\n",
    "                if acc is not None:\n",
    "                    accuracies.append(acc)\n",
    "        \n",
    "        if accuracies:\n",
    "            sorted_accs = np.sort(accuracies)\n",
    "            ecdf = np.arange(1, len(sorted_accs) + 1) / len(sorted_accs)\n",
    "            \n",
    "            color = colors.get(method, 'gray')\n",
    "            ax.step(sorted_accs, ecdf, where='post', color=color, linewidth=2.5,\n",
    "                   label=f\"{method.upper()} (n={len(accuracies)})\")\n",
    "            \n",
    "            # Add scatter points\n",
    "            ax.scatter(sorted_accs, ecdf, color=color, s=50, zorder=5)\n",
    "    \n",
    "    # Add threshold lines\n",
    "    for thresh in [0.7, 0.8, 0.9, 0.95]:\n",
    "        ax.axvline(x=thresh, color='red', linestyle='--', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('Final Classification Accuracy', fontsize=12)\n",
    "    ax.set_ylabel('Cumulative Probability', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(loc='lower right', fontsize=11)\n",
    "    ax.set_xlim(0.5, 1.02)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage (uncomment when metrics are computed)\n",
    "# fig = plot_accuracy_ecdf(metrics)\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when metrics are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot: Pareto Front (Accuracy vs Circuit Depth)\n",
    "\n",
    "Trade-off between classification accuracy and circuit complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto_accuracy_vs_depth(metrics, title=\"Pareto: Accuracy vs Circuit Depth\"):\n",
    "    \"\"\"\n",
    "    Plot Pareto frontier of accuracy vs circuit depth/gate count.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Aggregated metrics from aggregate_classification_metrics\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    \n",
    "    colors = {'drl': '#1f77b4', 'ea': '#ff7f0e'}\n",
    "    markers = {'drl': 'o', 'ea': 's'}\n",
    "    \n",
    "    for method in ['drl', 'ea']:\n",
    "        accuracies = []\n",
    "        depths = []\n",
    "        \n",
    "        for run_key, run_data in metrics.get('per_run', {}).items():\n",
    "            if run_data['method'] == method:\n",
    "                acc = run_data.get('best_test_accuracy')\n",
    "                depth = run_data.get('best_model_gate_count') or run_data.get('final_gate_count')\n",
    "                \n",
    "                if acc is not None and depth is not None:\n",
    "                    accuracies.append(acc)\n",
    "                    depths.append(depth)\n",
    "        \n",
    "        if accuracies:\n",
    "            color = colors.get(method, 'gray')\n",
    "            marker = markers.get(method, 'o')\n",
    "            ax.scatter(depths, accuracies, c=color, marker=marker, s=120, alpha=0.7,\n",
    "                      label=f\"{method.upper()} (n={len(accuracies)})\", edgecolors='white', linewidth=1)\n",
    "    \n",
    "    # Add accuracy threshold\n",
    "    ax.axhline(y=0.9, color='green', linestyle='--', alpha=0.5, linewidth=1.5, label='90% threshold')\n",
    "    \n",
    "    ax.set_xlabel('Gate Count', fontsize=12)\n",
    "    ax.set_ylabel('Best Classification Accuracy', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.legend(loc='lower right', fontsize=11)\n",
    "    ax.set_ylim(0.5, 1.02)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "# fig = plot_pareto_accuracy_vs_depth(metrics)\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when metrics are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot: Box/Bar Plot of Final Accuracies\n",
    "\n",
    "Statistical comparison of final accuracies across methods and seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_boxplot(metrics, title=\"Final Classification Accuracies by Method\"):\n",
    "    \"\"\"\n",
    "    Create box plot comparing final accuracies across methods.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Aggregated metrics from aggregate_classification_metrics\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return None\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    colors = {'drl': '#1f77b4', 'ea': '#ff7f0e'}\n",
    "    \n",
    "    # Extract accuracies by method\n",
    "    data_by_method = {'drl': [], 'ea': []}\n",
    "    \n",
    "    for run_key, run_data in metrics.get('per_run', {}).items():\n",
    "        method = run_data['method']\n",
    "        acc = run_data.get('final_test_accuracy') or run_data.get('best_test_accuracy')\n",
    "        if acc is not None and method in data_by_method:\n",
    "            data_by_method[method].append(acc)\n",
    "    \n",
    "    # Box plot\n",
    "    bp = ax1.boxplot([data_by_method['drl'], data_by_method['ea']],\n",
    "                    labels=['DRL', 'EA'], patch_artist=True)\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], [colors['drl'], colors['ea']]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax1.set_ylabel('Final Classification Accuracy', fontsize=12)\n",
    "    ax1.set_title('Box Plot', fontsize=14)\n",
    "    ax1.set_ylim(0.5, 1.02)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    ax1.axhline(y=0.9, color='green', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Bar plot with error bars\n",
    "    methods = ['DRL', 'EA']\n",
    "    means = [\n",
    "        np.mean(data_by_method['drl']) if data_by_method['drl'] else 0,\n",
    "        np.mean(data_by_method['ea']) if data_by_method['ea'] else 0\n",
    "    ]\n",
    "    stds = [\n",
    "        np.std(data_by_method['drl'], ddof=1) if len(data_by_method['drl']) > 1 else 0,\n",
    "        np.std(data_by_method['ea'], ddof=1) if len(data_by_method['ea']) > 1 else 0\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    bars = ax2.bar(x, means, yerr=stds, capsize=5, color=[colors['drl'], colors['ea']], alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, mean, std in zip(bars, means, stds):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01,\n",
    "                f'{mean:.3f}±{std:.3f}', ha='center', va='bottom', fontsize=11)\n",
    "    \n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(methods)\n",
    "    ax2.set_ylabel('Final Classification Accuracy', fontsize=12)\n",
    "    ax2.set_title('Bar Plot (mean ± std)', fontsize=14)\n",
    "    ax2.set_ylim(0.5, 1.1)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.axhline(y=0.9, color='green', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage\n",
    "# fig = plot_accuracy_boxplot(metrics)\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when metrics are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison Checklist\n",
    "\n",
    "Verify that DRL and EA experiments use matched settings for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checklist for matching DRL and EA settings\n",
    "print(\"=\"*60)\n",
    "print(\"FAIR COMPARISON CHECKLIST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checklist = [\n",
    "    (\"Gate set\", \"RX, RY, RZ, CNOT\", \"Match paper's allowed gates\"),\n",
    "    (\"Max circuit depth/gates\", \"L=20\", \"Match paper's L parameter\"),\n",
    "    (\"Evaluation budget\", \"~800 episodes/evaluations\", \"Match paper's episode count\"),\n",
    "    (\"Inner-loop optimization\", \"15 epochs max, Adam\", \"Match paper's VQC training\"),\n",
    "    (\"Dataset\", \"make_classification or make_moons\", \"Match paper's datasets\"),\n",
    "    (\"Train/test split\", \"80/20 (assumed)\", \"Use consistent split\"),\n",
    "    (\"Data encoding\", \"arctan embedding\", \"Match paper's encoding\"),\n",
    "    (\"Number of seeds\", \"≥5\", \"Statistical validity\"),\n",
    "    (\"Gate penalty coefficient\", \"0.01\", \"Match paper's complexity penalty\"),\n",
    "    (\"Number of qubits\", \"4 (matches features)\", \"Match data dimensionality\"),\n",
    "]\n",
    "\n",
    "for item, value, note in checklist:\n",
    "    print(f\"\\n☐ {item}\")\n",
    "    print(f\"   Value: {value}\")\n",
    "    print(f\"   Note: {note}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Verify configs in comparison/experiments/configs/ match these settings\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics summary\n",
    "output_dir = Path(\"../logs/classification_analysis\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Uncomment when logs are available:\n",
    "# json_path, csv_path = save_classification_summary(metrics, output_dir)\n",
    "# print(f\"Saved JSON: {json_path}\")\n",
    "# print(f\"Saved CSV: {csv_path}\")\n",
    "\n",
    "# Save plots\n",
    "# if HAS_PLOTTING:\n",
    "#     fig = plot_accuracy_vs_evals({'drl': valid_drl, 'ea': valid_ea})\n",
    "#     fig.savefig(output_dir / 'accuracy_vs_evals.png', dpi=150, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "#     \n",
    "#     fig = plot_accuracy_ecdf(metrics)\n",
    "#     fig.savefig(output_dir / 'accuracy_ecdf.png', dpi=150, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "#     \n",
    "#     fig = plot_pareto_accuracy_vs_depth(metrics)\n",
    "#     fig.savefig(output_dir / 'pareto_accuracy_depth.png', dpi=150, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "#     \n",
    "#     fig = plot_accuracy_boxplot(metrics)\n",
    "#     fig.savefig(output_dir / 'accuracy_boxplot.png', dpi=150, bbox_inches='tight')\n",
    "#     plt.close(fig)\n",
    "#     \n",
    "#     print(f\"Saved plots to: {output_dir}\")\n",
    "\n",
    "print(\"TODO: Uncomment save code when analysis is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Insights from Paper Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KEY INSIGHTS FROM PAPER (arXiv:2407.20147)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. REWARD FUNCTION:\")\n",
    "reward = paper_metadata.get('reward_function', {})\n",
    "print(f\"   Type: {reward.get('type')}\")\n",
    "print(f\"   Gate penalty: {reward.get('gate_penalty_coefficient')}\")\n",
    "print(f\"   Reward clipping: {reward.get('reward_clip_range')}\")\n",
    "\n",
    "print(\"\\n2. ADAPTIVE SEARCH:\")\n",
    "adaptive = paper_metadata.get('adaptive_search', {})\n",
    "print(f\"   ytarget increment: {adaptive.get('ytarget_increment')}\")\n",
    "print(f\"   Epsilon decay on increase: {adaptive.get('epsilon_decay_on_increase')}\")\n",
    "\n",
    "print(\"\\n3. OMITTED HYPERPARAMETERS (need assumptions):\")\n",
    "omissions = paper_metadata.get('notes_and_omissions', {}).get('omitted_hyperparameters', [])\n",
    "for item in omissions[:5]:\n",
    "    print(f\"   - {item}\")\n",
    "\n",
    "print(\"\\n4. KEY INSIGHTS FOR REPRODUCTION:\")\n",
    "insights = paper_metadata.get('notes_and_omissions', {}).get('key_insights', [])\n",
    "for item in insights:\n",
    "    print(f\"   - {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Implement classification task** in EA pipeline (adapt existing VQE environment)\n",
    "2. **Implement DRL agent** following the DDQN specification from the paper\n",
    "3. **Run experiments** with matched settings using the YAML configs\n",
    "4. **Generate logs** following the schema in `comparison/logs/schema.json`\n",
    "5. **Analyze results** by uncommenting the code in this notebook\n",
    "\n",
    "### Example Commands\n",
    "\n",
    "```bash\n",
    "# Run classification metrics analysis\n",
    "python -m comparison.analysis.compute_classif_metrics \\\n",
    "    --input \"comparison/logs/**/*.jsonl\" \\\n",
    "    --out comparison/logs/classification_analysis\n",
    "\n",
    "# Run tests\n",
    "pytest comparison/tests/ -v\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
