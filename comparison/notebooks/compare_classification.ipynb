{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL vs EA Classification Comparison\n",
    "\n",
    "This notebook provides a scaffold for comparing the DRL approach from arXiv:2407.20147\n",
    "with the coevolutionary (EA) agents on **classification tasks**.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Run experiments and generate logs using the configs in `comparison/experiments/configs/`\n",
    "   - `drl_classification.yaml` for DRL experiments\n",
    "   - `ea_classification.yaml` for EA experiments\n",
    "2. Place log files in `comparison/logs/` following the naming convention:\n",
    "   - `drl_classif_run_{seed}.jsonl` for DRL results\n",
    "   - `ea_classif_run_{seed}.jsonl` for EA results\n",
    "3. Install dependencies: `pip install -r comparison/requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment Checklist\n",
    "\n",
    "Before running comparison, verify the following settings are aligned:\n",
    "\n",
    "### Gate Set\n",
    "- [ ] Both methods use same gate set: RX, RY, RZ, CNOT\n",
    "- [ ] CNOT connectivity matches (nearest-neighbor cyclic)\n",
    "\n",
    "### Inner-Loop Optimization\n",
    "- [ ] Same loss function (binary cross-entropy)\n",
    "- [ ] Same number of epochs per step (15 for make_classification, 25 for make_moons)\n",
    "- [ ] Same data encoding (arctan embedding)\n",
    "\n",
    "### Circuit Constraints\n",
    "- [ ] Same max_depth/max_gates (20 for make_classification, 25 for make_moons)\n",
    "- [ ] Same number of qubits (4 for make_classification, 2 for make_moons)\n",
    "\n",
    "### Evaluation Budget\n",
    "- [ ] Comparable total circuit evaluations (~1200 episodes/evaluations)\n",
    "- [ ] Same seeds used for reproducibility\n",
    "\n",
    "### Dataset\n",
    "- [ ] Same dataset (make_classification or make_moons)\n",
    "- [ ] Same train/test split (if specified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repository root to path\n",
    "repo_root = Path().resolve().parent.parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import classification metrics module\n",
    "from comparison.analysis.compute_classif_metrics import (\n",
    "    load_logs,\n",
    "    aggregate_classification_metrics,\n",
    "    save_summary,\n",
    ")\n",
    "\n",
    "# Optional: Import visualization libraries\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    HAS_PLOTTING = True\n",
    "except ImportError:\n",
    "    HAS_PLOTTING = False\n",
    "    print(\"matplotlib/numpy not available. Install with: pip install matplotlib numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Paper Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load paper metadata\n",
    "METADATA_PATH = Path(\"../paper_metadata/quantum_ml_arch_search_2407.20147.json\")\n",
    "\n",
    "if METADATA_PATH.exists():\n",
    "    with open(METADATA_PATH) as f:\n",
    "        paper_metadata = json.load(f)\n",
    "    print(f\"Paper: {paper_metadata['paper_title']}\")\n",
    "    print(f\"Authors: {', '.join(paper_metadata['authors'])}\")\n",
    "    print(f\"arXiv: {paper_metadata['arxiv_id']}\")\n",
    "    print(f\"\\nDatasets: {[d['name'] for d in paper_metadata['tasks']['datasets']]}\")\n",
    "else:\n",
    "    print(f\"Metadata file not found at {METADATA_PATH}\")\n",
    "    paper_metadata = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Experiment Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure log paths\n",
    "LOGS_DIR = Path(\"../logs\")\n",
    "\n",
    "# Load DRL classification logs\n",
    "drl_log_pattern = str(LOGS_DIR / \"drl\" / \"*classif*.jsonl\")\n",
    "drl_logs = load_logs(drl_log_pattern)\n",
    "print(f\"Loaded {len(drl_logs)} DRL classification log entries\")\n",
    "\n",
    "# Load EA classification logs\n",
    "ea_log_pattern = str(LOGS_DIR / \"ea\" / \"*classif*.jsonl\")\n",
    "ea_logs = load_logs(ea_log_pattern)\n",
    "print(f\"Loaded {len(ea_logs)} EA classification log entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine logs for comparison\n",
    "all_logs = drl_logs + ea_logs\n",
    "\n",
    "# Compute classification metrics with custom thresholds\n",
    "thresholds = [0.70, 0.80, 0.90]\n",
    "metrics = aggregate_classification_metrics(all_logs, thresholds=thresholds)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Total runs: {metrics['total_runs']}\")\n",
    "print(f\"Total log entries: {metrics['total_logs']}\")\n",
    "print(f\"Thresholds: {metrics['thresholds_used']}\")\n",
    "\n",
    "print(\"\\n--- By Method ---\")\n",
    "for method, stats in metrics.get('by_method', {}).items():\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    print(f\"  Runs: {stats['n_runs']}\")\n",
    "    if stats['mean_max_val_accuracy'] is not None:\n",
    "        std = stats.get('std_max_val_accuracy', 0) or 0\n",
    "        print(f\"  Max val accuracy: {stats['mean_max_val_accuracy']:.4f} ± {std:.4f}\")\n",
    "    if stats['mean_final_val_accuracy'] is not None:\n",
    "        std = stats.get('std_final_val_accuracy', 0) or 0\n",
    "        print(f\"  Final val accuracy: {stats['mean_final_val_accuracy']:.4f} ± {std:.4f}\")\n",
    "    if stats['mean_final_test_accuracy'] is not None:\n",
    "        std = stats.get('std_final_test_accuracy', 0) or 0\n",
    "        print(f\"  Final test accuracy: {stats['mean_final_test_accuracy']:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation/Test Accuracy vs Evaluations\n",
    "\n",
    "Plot learning curves showing accuracy improvement over training evaluations (median ± CI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_vs_evals(logs_by_method, acc_key='best_val_accuracy', \n",
    "                           title=\"Validation Accuracy vs Evaluations\"):\n",
    "    \"\"\"\n",
    "    Plot accuracy learning curves for each method with median and confidence interval.\n",
    "    \n",
    "    Args:\n",
    "        logs_by_method: dict mapping method name to list of log entries\n",
    "        acc_key: key for accuracy value in log entries\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = {'drl': 'blue', 'ea': 'orange'}\n",
    "    \n",
    "    for method, logs in logs_by_method.items():\n",
    "        # Group by seed\n",
    "        seeds = {}\n",
    "        for log in logs:\n",
    "            seed = log.get('seed', 0)\n",
    "            if seed not in seeds:\n",
    "                seeds[seed] = {'evals': [], 'accs': []}\n",
    "            \n",
    "            eval_count = log.get('cum_eval_count') or log.get('eval_id', 0)\n",
    "            acc = log.get(acc_key) or log.get('best_fidelity', 0)\n",
    "            \n",
    "            seeds[seed]['evals'].append(eval_count)\n",
    "            seeds[seed]['accs'].append(acc)\n",
    "        \n",
    "        color = colors.get(method, 'gray')\n",
    "        \n",
    "        # Plot individual runs with light color\n",
    "        for seed, data in seeds.items():\n",
    "            ax.plot(data['evals'], data['accs'], \n",
    "                   color=color, alpha=0.2, linewidth=1)\n",
    "        \n",
    "        # TODO: Compute and plot median with CI\n",
    "        # This requires interpolating accuracies to common eval points\n",
    "    \n",
    "    ax.set_xlabel('Cumulative Evaluations')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(list(logs_by_method.keys()))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot (uncomment when logs are available)\n",
    "# fig = plot_accuracy_vs_evals({'drl': drl_logs, 'ea': ea_logs})\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when logs are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ECDF of Final Accuracies\n",
    "\n",
    "Plot the empirical cumulative distribution function of final validation/test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ecdf(fidelities_by_method, title=\"ECDF of Final Accuracies\"):\n",
    "    \"\"\"\n",
    "    Plot empirical CDF of final accuracies for each method.\n",
    "    \n",
    "    Args:\n",
    "        fidelities_by_method: dict mapping method name to list of final accuracies\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    colors = {'drl': 'blue', 'ea': 'orange'}\n",
    "    \n",
    "    for method, accs in fidelities_by_method.items():\n",
    "        if not accs:\n",
    "            continue\n",
    "        \n",
    "        sorted_accs = np.sort(accs)\n",
    "        ecdf = np.arange(1, len(sorted_accs) + 1) / len(sorted_accs)\n",
    "        \n",
    "        color = colors.get(method, 'gray')\n",
    "        ax.step(sorted_accs, ecdf, where='post', \n",
    "               color=color, linewidth=2, label=f\"{method} (n={len(accs)})\")\n",
    "    \n",
    "    ax.set_xlabel('Final Accuracy')\n",
    "    ax.set_ylabel('Cumulative Probability')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0.5, 1.05)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Add threshold lines\n",
    "    for thresh in [0.70, 0.80, 0.90]:\n",
    "        ax.axvline(x=thresh, color='red', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Extract final accuracies from per_run metrics (uncomment when available)\n",
    "# drl_accs = [r['final_val_accuracy'] for r in metrics['per_run'].values() \n",
    "#             if r['method'] == 'drl' and r['final_val_accuracy'] is not None]\n",
    "# ea_accs = [r['final_val_accuracy'] for r in metrics['per_run'].values() \n",
    "#            if r['method'] == 'ea' and r['final_val_accuracy'] is not None]\n",
    "# fig = plot_ecdf({'drl': drl_accs, 'ea': ea_accs})\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when metrics are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pareto Plot: Accuracy vs Depth/Gate Count\n",
    "\n",
    "Plot the Pareto frontier of accuracy vs circuit complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto_accuracy_vs_depth(runs_by_method, title=\"Pareto: Accuracy vs Circuit Depth\"):\n",
    "    \"\"\"\n",
    "    Plot Pareto frontier of accuracy vs circuit depth.\n",
    "    \n",
    "    Args:\n",
    "        runs_by_method: dict mapping method name to list of run dicts with\n",
    "                       'max_val_accuracy' and 'final_depth'/'min_depth' keys\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    colors = {'drl': 'blue', 'ea': 'orange'}\n",
    "    markers = {'drl': 'o', 'ea': 's'}\n",
    "    \n",
    "    for method, runs in runs_by_method.items():\n",
    "        accs = []\n",
    "        depths = []\n",
    "        \n",
    "        for run in runs:\n",
    "            acc = run.get('max_val_accuracy') or run.get('final_val_accuracy')\n",
    "            depth = run.get('final_depth') or run.get('min_depth') or run.get('final_gate_count')\n",
    "            if acc is not None and depth is not None:\n",
    "                accs.append(acc)\n",
    "                depths.append(depth)\n",
    "        \n",
    "        if accs:\n",
    "            color = colors.get(method, 'gray')\n",
    "            marker = markers.get(method, 'o')\n",
    "            ax.scatter(depths, accs, \n",
    "                      c=color, marker=marker, s=100, alpha=0.7,\n",
    "                      label=f\"{method} (n={len(accs)})\")\n",
    "    \n",
    "    ax.set_xlabel('Circuit Depth / Gate Count')\n",
    "    ax.set_ylabel('Max Validation Accuracy')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0.5, 1.05)\n",
    "    \n",
    "    # Add accuracy threshold line\n",
    "    ax.axhline(y=0.90, color='green', linestyle='--', alpha=0.5, label='90% threshold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot (uncomment when metrics are available)\n",
    "# drl_runs = [r for r in metrics['per_run'].values() if r['method'] == 'drl']\n",
    "# ea_runs = [r for r in metrics['per_run'].values() if r['method'] == 'ea']\n",
    "# fig = plot_pareto_accuracy_vs_depth({'drl': drl_runs, 'ea': ea_runs})\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when metrics are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Boxplots of Final Accuracies\n",
    "\n",
    "Compare distributions of final accuracies across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplots(accs_by_method, title=\"Final Accuracy Distribution\"):\n",
    "    \"\"\"\n",
    "    Plot boxplots of final accuracies for each method.\n",
    "    \n",
    "    Args:\n",
    "        accs_by_method: dict mapping method name to list of final accuracies\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return None\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    methods = list(accs_by_method.keys())\n",
    "    data = [accs_by_method[m] for m in methods]\n",
    "    colors = ['blue', 'orange']\n",
    "    \n",
    "    bp = ax.boxplot(data, labels=methods, patch_artist=True)\n",
    "    for patch, color in zip(bp['boxes'], colors[:len(methods)]):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.5)\n",
    "    \n",
    "    ax.set_ylabel('Final Accuracy')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0.5, 1.05)\n",
    "    \n",
    "    # Add threshold lines\n",
    "    for thresh in [0.70, 0.80, 0.90]:\n",
    "        ax.axhline(y=thresh, color='red', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot (uncomment when metrics are available)\n",
    "# drl_accs = [r['final_val_accuracy'] for r in metrics['per_run'].values() \n",
    "#             if r['method'] == 'drl' and r['final_val_accuracy'] is not None]\n",
    "# ea_accs = [r['final_val_accuracy'] for r in metrics['per_run'].values() \n",
    "#            if r['method'] == 'ea' and r['final_val_accuracy'] is not None]\n",
    "# fig = plot_boxplots({'DRL': drl_accs, 'EA': ea_accs})\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when metrics are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics summary\n",
    "output_dir = Path(\"../logs/classif_analysis\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Uncomment when logs are available:\n",
    "# json_path, csv_path = save_summary(metrics, output_dir)\n",
    "# print(f\"Saved JSON: {json_path}\")\n",
    "# print(f\"Saved CSV: {csv_path}\")\n",
    "\n",
    "# Save plots\n",
    "# if HAS_PLOTTING:\n",
    "#     fig = plot_accuracy_vs_evals({'drl': drl_logs, 'ea': ea_logs})\n",
    "#     fig.savefig(output_dir / 'accuracy_vs_evals.png', dpi=150)\n",
    "#     plt.close(fig)\n",
    "#     print(f\"Saved: {output_dir / 'accuracy_vs_evals.png'}\")\n",
    "\n",
    "print(\"TODO: Uncomment save code when analysis is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Generate logs**: Run experiments using the configs in `comparison/experiments/configs/`\n",
    "   - For DRL: Configure `drl_classification.yaml` and run your DRL agent\n",
    "   - For EA: Configure `ea_classification.yaml` and run the repository's EA pipeline\n",
    "\n",
    "2. **Update paths**: Modify the log paths in this notebook to point to your results\n",
    "\n",
    "3. **Analyze results**: Uncomment the plotting and analysis code\n",
    "\n",
    "4. **Compare methods**: Look at the metrics summary to compare DRL vs EA\n",
    "\n",
    "### Example Commands\n",
    "\n",
    "```bash\n",
    "# Run EA classification experiments (from repo root)\n",
    "# TODO: Replace with actual EA runner command\n",
    "python run_experiments.py --preset quick --n-qubits 4 --seed 42\n",
    "\n",
    "# Compute classification metrics from logs\n",
    "python -m comparison.analysis.compute_classif_metrics \\\n",
    "    --input \"comparison/logs/**/*classif*.jsonl\" \\\n",
    "    --out comparison/logs/classif_analysis \\\n",
    "    --thresholds 0.70 0.80 0.90\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
