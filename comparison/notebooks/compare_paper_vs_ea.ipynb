{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL vs EA Quantum Architecture Search Comparison\n",
    "\n",
    "This notebook provides a scaffold for comparing the DRL approach from arXiv:2407.20147\n",
    "with the coevolutionary (EA) agents in this repository.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Run experiments and generate logs using the configs in `comparison/experiments/configs/`\n",
    "2. Place log files in `comparison/logs/` following the naming convention:\n",
    "   - `drl_run_{seed}.jsonl` for DRL results\n",
    "   - `ea_run_{seed}.jsonl` for EA results\n",
    "3. Install dependencies: `pip install -r comparison/requirements.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add repository root to path\n",
    "repo_root = Path().resolve().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import comparison modules\n",
    "from comparison.analysis.compute_metrics import (\n",
    "    load_logs,\n",
    "    validate_logs,\n",
    "    aggregate_metrics,\n",
    "    save_summary,\n",
    ")\n",
    "from comparison.diagnostics.diagnose_fidelity import (\n",
    "    compute_fidelities,\n",
    "    run_basic_sanity_checks,\n",
    ")\n",
    "\n",
    "# Optional: Import visualization libraries\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    HAS_PLOTTING = True\n",
    "except ImportError:\n",
    "    HAS_PLOTTING = False\n",
    "    print(\"matplotlib/numpy not available. Install with: pip install matplotlib numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Experiment Logs\n",
    "\n",
    "Load logs from `comparison/logs/` directory. Update paths as needed for your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure log paths\n",
    "LOGS_DIR = Path(\"../logs\")\n",
    "\n",
    "# Load DRL logs (placeholder - update with actual paths)\n",
    "drl_log_pattern = str(LOGS_DIR / \"drl\" / \"*.jsonl\")\n",
    "drl_logs = load_logs(drl_log_pattern)\n",
    "print(f\"Loaded {len(drl_logs)} DRL log entries\")\n",
    "\n",
    "# Load EA logs (placeholder - update with actual paths)\n",
    "ea_log_pattern = str(LOGS_DIR / \"ea\" / \"*.jsonl\")\n",
    "ea_logs = load_logs(ea_log_pattern)\n",
    "print(f\"Loaded {len(ea_logs)} EA log entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate Logs Against Schema\n",
    "\n",
    "Ensure log entries conform to the expected schema for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate DRL logs\n",
    "valid_drl, drl_errors = validate_logs(drl_logs)\n",
    "print(f\"DRL: {len(valid_drl)} valid, {len(drl_errors)} errors\")\n",
    "\n",
    "# Validate EA logs\n",
    "valid_ea, ea_errors = validate_logs(ea_logs)\n",
    "print(f\"EA: {len(valid_ea)} valid, {len(ea_errors)} errors\")\n",
    "\n",
    "# Show first few errors if any\n",
    "if drl_errors:\n",
    "    print(\"\\nDRL validation errors (first 3):\")\n",
    "    for idx, err in drl_errors[:3]:\n",
    "        print(f\"  Entry {idx}: {err}\")\n",
    "\n",
    "if ea_errors:\n",
    "    print(\"\\nEA validation errors (first 3):\")\n",
    "    for idx, err in ea_errors[:3]:\n",
    "        print(f\"  Entry {idx}: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute Aggregated Metrics\n",
    "\n",
    "Compute per-run and cross-run statistics for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine logs for comparison\n",
    "all_logs = valid_drl + valid_ea\n",
    "\n",
    "# Compute aggregated metrics\n",
    "metrics = aggregate_metrics(all_logs)\n",
    "\n",
    "# Display summary\n",
    "print(f\"Total runs: {metrics['total_runs']}\")\n",
    "print(f\"Total log entries: {metrics['total_logs']}\")\n",
    "\n",
    "print(\"\\n--- By Method ---\")\n",
    "for method, stats in metrics.get('by_method', {}).items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Runs: {stats['n_runs']}\")\n",
    "    if stats['mean_max_fidelity'] is not None:\n",
    "        print(f\"  Mean max fidelity: {stats['mean_max_fidelity']:.4f} ± {stats.get('std_max_fidelity', 0):.4f}\")\n",
    "    if stats['mean_final_fidelity'] is not None:\n",
    "        print(f\"  Mean final fidelity: {stats['mean_final_fidelity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fidelity vs Evaluations Plot\n",
    "\n",
    "Plot learning curves showing fidelity improvement over training evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fidelity_vs_evals(logs_by_method, title=\"Fidelity vs Evaluations\"):\n",
    "    \"\"\"\n",
    "    Plot fidelity learning curves for each method.\n",
    "    \n",
    "    Args:\n",
    "        logs_by_method: dict mapping method name to list of log entries\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = {'drl': 'blue', 'ea': 'orange'}\n",
    "    \n",
    "    for method, logs in logs_by_method.items():\n",
    "        # Group by seed and plot individual runs\n",
    "        seeds = {}\n",
    "        for log in logs:\n",
    "            seed = log.get('seed', 0)\n",
    "            if seed not in seeds:\n",
    "                seeds[seed] = {'evals': [], 'fidelities': []}\n",
    "            seeds[seed]['evals'].append(log.get('cum_eval_count', 0))\n",
    "            seeds[seed]['fidelities'].append(log.get('best_fidelity', 0))\n",
    "        \n",
    "        # Plot each seed with light color\n",
    "        color = colors.get(method, 'gray')\n",
    "        for seed, data in seeds.items():\n",
    "            ax.plot(data['evals'], data['fidelities'], \n",
    "                   color=color, alpha=0.3, linewidth=1)\n",
    "        \n",
    "        # TODO: Add mean line with standard deviation shading\n",
    "        # This requires aligning evaluation counts across seeds\n",
    "    \n",
    "    ax.set_xlabel('Cumulative Evaluations')\n",
    "    ax.set_ylabel('Best Fidelity')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(list(logs_by_method.keys()))\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage (uncomment when logs are available)\n",
    "# fig = plot_fidelity_vs_evals({'drl': valid_drl, 'ea': valid_ea})\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when logs are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Empirical CDF (ECDF) Plot\n",
    "\n",
    "Plot the empirical cumulative distribution function of final fidelities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ecdf(fidelities_by_method, title=\"ECDF of Final Fidelities\"):\n",
    "    \"\"\"\n",
    "    Plot empirical CDF of final fidelities for each method.\n",
    "    \n",
    "    Args:\n",
    "        fidelities_by_method: dict mapping method name to list of final fidelities\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    colors = {'drl': 'blue', 'ea': 'orange'}\n",
    "    \n",
    "    for method, fidelities in fidelities_by_method.items():\n",
    "        if not fidelities:\n",
    "            continue\n",
    "        \n",
    "        sorted_fids = np.sort(fidelities)\n",
    "        ecdf = np.arange(1, len(sorted_fids) + 1) / len(sorted_fids)\n",
    "        \n",
    "        color = colors.get(method, 'gray')\n",
    "        ax.step(sorted_fids, ecdf, where='post', \n",
    "               color=color, linewidth=2, label=f\"{method} (n={len(fidelities)})\")\n",
    "    \n",
    "    ax.set_xlabel('Final Fidelity')\n",
    "    ax.set_ylabel('Cumulative Probability')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(0, 1.05)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Add threshold lines\n",
    "    for thresh in [0.9, 0.95, 0.99]:\n",
    "        ax.axvline(x=thresh, color='red', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage (uncomment when metrics are computed)\n",
    "# Extract final fidelities from per_run metrics\n",
    "# drl_fids = [r['final_fidelity'] for r in metrics['per_run'].values() \n",
    "#             if r['method'] == 'drl' and r['final_fidelity'] is not None]\n",
    "# ea_fids = [r['final_fidelity'] for r in metrics['per_run'].values() \n",
    "#            if r['method'] == 'ea' and r['final_fidelity'] is not None]\n",
    "# fig = plot_ecdf({'drl': drl_fids, 'ea': ea_fids})\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when metrics are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pareto Plot (Fidelity vs Gate Count)\n",
    "\n",
    "Plot the Pareto frontier of fidelity vs circuit complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pareto(runs_by_method, title=\"Pareto: Fidelity vs Gate Count\"):\n",
    "    \"\"\"\n",
    "    Plot Pareto frontier of fidelity vs gate count.\n",
    "    \n",
    "    Args:\n",
    "        runs_by_method: dict mapping method name to list of run dicts with\n",
    "                       'max_fidelity' and 'min_gate_count' keys\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    if not HAS_PLOTTING:\n",
    "        print(\"Plotting not available. Install matplotlib.\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    colors = {'drl': 'blue', 'ea': 'orange'}\n",
    "    markers = {'drl': 'o', 'ea': 's'}\n",
    "    \n",
    "    for method, runs in runs_by_method.items():\n",
    "        fidelities = []\n",
    "        gate_counts = []\n",
    "        \n",
    "        for run in runs:\n",
    "            fid = run.get('max_fidelity')\n",
    "            gc = run.get('min_gate_count') or run.get('final_gate_count')\n",
    "            if fid is not None and gc is not None:\n",
    "                fidelities.append(fid)\n",
    "                gate_counts.append(gc)\n",
    "        \n",
    "        if fidelities:\n",
    "            color = colors.get(method, 'gray')\n",
    "            marker = markers.get(method, 'o')\n",
    "            ax.scatter(gate_counts, fidelities, \n",
    "                      c=color, marker=marker, s=100, alpha=0.7,\n",
    "                      label=f\"{method} (n={len(fidelities)})\")\n",
    "    \n",
    "    ax.set_xlabel('Gate Count')\n",
    "    ax.set_ylabel('Max Fidelity')\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Add fidelity threshold line\n",
    "    ax.axhline(y=0.99, color='green', linestyle='--', alpha=0.5, label='0.99 threshold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage (uncomment when metrics are computed)\n",
    "# drl_runs = [r for r in metrics['per_run'].values() if r['method'] == 'drl']\n",
    "# ea_runs = [r for r in metrics['per_run'].values() if r['method'] == 'ea']\n",
    "# fig = plot_pareto({'drl': drl_runs, 'ea': ea_runs})\n",
    "# plt.show()\n",
    "print(\"TODO: Uncomment plotting code when metrics are available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results\n",
    "\n",
    "Save aggregated metrics and plots for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics summary\n",
    "output_dir = Path(\"../logs/analysis_output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Uncomment when logs are available:\n",
    "# json_path, csv_path = save_summary(metrics, output_dir)\n",
    "# print(f\"Saved JSON: {json_path}\")\n",
    "# print(f\"Saved CSV: {csv_path}\")\n",
    "\n",
    "# Save plots\n",
    "# if HAS_PLOTTING:\n",
    "#     fig = plot_fidelity_vs_evals({'drl': valid_drl, 'ea': valid_ea})\n",
    "#     fig.savefig(output_dir / 'fidelity_vs_evals.png', dpi=150)\n",
    "#     plt.close(fig)\n",
    "#     print(f\"Saved: {output_dir / 'fidelity_vs_evals.png'}\")\n",
    "\n",
    "print(\"TODO: Uncomment save code when analysis is complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Diagnostics Sanity Check\n",
    "\n",
    "Verify that the fidelity computation is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sanity checks\n",
    "sanity_results = run_basic_sanity_checks()\n",
    "\n",
    "print(\"Fidelity Sanity Checks:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for test_name, result in sanity_results.items():\n",
    "    print(f\"\\n{test_name}:\")\n",
    "    print(f\"  Raw trace fidelity: {result['raw_trace_f']:.6f}\")\n",
    "    print(f\"  Phase-corrected trace fidelity: {result['phase_corrected_trace_f']:.6f}\")\n",
    "    print(f\"  Average gate fidelity: {result['Favg']:.6f}\")\n",
    "\n",
    "# Verify expected results\n",
    "assert abs(sanity_results['identity_identity']['raw_trace_f'] - 1.0) < 1e-10\n",
    "assert abs(sanity_results['toffoli_toffoli']['raw_trace_f'] - 1.0) < 1e-10\n",
    "assert abs(sanity_results['toffoli_toffoli_phase']['phase_corrected_trace_f'] - 1.0) < 1e-10\n",
    "print(\"\\n✓ All sanity checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Generate logs**: Run experiments using the configs in `comparison/experiments/configs/`\n",
    "2. **Update paths**: Modify the log paths in this notebook to point to your results\n",
    "3. **Analyze results**: Uncomment the plotting and analysis code\n",
    "4. **Compare methods**: Look at the metrics summary to compare DRL vs EA\n",
    "\n",
    "### Example Commands\n",
    "\n",
    "```bash\n",
    "# Run EA experiments (from repo root)\n",
    "python run_experiments.py --preset quick --n-qubits 3 --seed 42\n",
    "\n",
    "# Compute metrics from logs\n",
    "python -m comparison.analysis.compute_metrics --input \"comparison/logs/**/*.jsonl\" --out comparison/logs/analysis_output\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
